# Monte Carlo Localization for Indoor Robot Navigation

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![ROS2](https://img.shields.io/badge/ROS2-Humble-blue.svg)](https://docs.ros.org/en/humble/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Gazebo](https://img.shields.io/badge/Gazebo-11-orange.svg)](https://gazebosim.org/)

**Particle filter-based robot localization using LiDAR in GPS-denied environments - UIUC ECE 484 Safe Autonomy**

## ğŸ¯ Key Results

- âœ… **Sub-meter Accuracy** - Average localization error < 0.5m after convergence
- âœ… **Fast Convergence** - Particles converge within 50-100 iterations
- âœ… **Real-time Performance** - Runs at 10 Hz with 1500 particles
- âœ… **8-Direction Sensing** - Enhanced LiDAR processing for improved accuracy
- âœ… **GPS-Denied Operation** - Fully autonomous indoor localization

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [System Architecture](#system-architecture)
- [Algorithm Details](#algorithm-details)
- [Installation](#installation)
- [Usage](#usage)
- [Experimental Results](#experimental-results)
- [Performance Analysis](#performance-analysis)
- [Course Context](#course-context)
- [Team](#team)

---

## Overview

This project implements **Monte Carlo Localization (MCL)** - a particle filter-based probabilistic algorithm for robot localization. The system localizes a mobile robot navigating the first floor of ECEB at UIUC using LiDAR measurements and a known map, without GPS.

### Problem Statement

**Given:**
- Detailed 2D occupancy grid map of ECEB
- LiDAR sensor measurements (8 directions)
- Odometry from robot control inputs

**Find:**
- Robot's position (x, y) and orientation (Î¸) in real-time

### Why Particle Filters?

- âœ… **Non-parametric** - Represents multi-modal distributions
- âœ… **Robust** - Handles non-Gaussian noise and non-linear dynamics
- âœ… **Efficient** - Parallelizable and scalable
- âœ… **Versatile** - Works with any sensor model

---

## ğŸ—ï¸ System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gazebo Simulatorâ”‚
â”‚   (ECEB Map)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Raw Point Cloud
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LiDAR Processingâ”‚â”€â”€â”€â”€â”€â–¶â”‚ Sensor Model â”‚
â”‚  (8 directions) â”‚      â”‚  (Particles) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
         â”‚ Measurements         â”‚ Predicted
         â”‚                      â”‚ Measurements
         â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Monte Carlo Localization       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Motion Update           â”‚  â”‚
â”‚  â”‚ 2. Measurement Update      â”‚  â”‚
â”‚  â”‚ 3. Weight Calculation      â”‚  â”‚
â”‚  â”‚ 4. Resampling              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Estimated    â”‚
         â”‚  Pose (x,y,Î¸) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

**1. LiDAR Processing (`lidar_processing.py`)**
- Subscribes to raw point cloud from Gazebo
- Processes 360Â° scan into 8 directional measurements
- Directions: Front (0Â°), Front-Right (45Â°), Right (90Â°), Rear-Right (135Â°), Rear (180Â°), Rear-Left (225Â°), Left (270Â°), Front-Left (315Â°)
- Returns distances to nearest obstacles in each direction

**2. Sensor Model (`maze.py`)**
- Ray-casting on 2D occupancy grid map
- Simulates LiDAR measurements for each particle
- Sensor range limit: 15m (configurable)

**3. Particle Filter (`particle_filter.py`)**
- Maintains distribution of 250-1500 particles representing possible robot poses
- Implements MCL algorithm with motion and measurement updates
- Adaptive resampling with GPS integration for bonus problem

**4. Vehicle Controller (`vehicle.py`)**
- Provides odometry/control signals
- Follows pre-defined waypoints through ECEB

---

## ğŸ”¬ Algorithm Details

### Monte Carlo Localization Algorithm

#### 1. **Initialization**
```python
# Uniformly distribute N particles in free space
particles = []
for i in range(N):
    x = random(0, map_width)
    y = random(0, map_height)
    theta = random(0, 2Ï€)
    weight = 1/N
    particles.append(Particle(x, y, theta, weight))
```

#### 2. **Motion Update (Prediction)**

Given control input `u = [v, Î´]` (velocity, steering angle), propagate each particle using vehicle dynamics:
```python
# Vehicle dynamics model
dx/dt = v * cos(Î¸)
dy/dt = v * sin(Î¸)
dÎ¸/dt = Î´

# Integrate using scipy.integrate.ode with dt = 0.01s
# Add motion noise for diversity
x_new += random_normal(0, Ïƒ_x=0.02)
y_new += random_normal(0, Ïƒ_y=0.02)
Î¸_new += random_normal(0, Ïƒ_Î¸=0.9)
```

#### 3. **Measurement Update (Correction)**

Calculate particle weights using Gaussian likelihood:
```python
for particle in particles:
    # Get predicted LiDAR readings for this particle
    z_predicted = sensor_model(particle.pose)
    
    # Compare with actual sensor reading z_actual
    squared_diff = sum((z_actual[i] - z_predicted[i])^2)
    
    # Gaussian weight with Ïƒ = 2.0
    weight = exp(-squared_diff / (2 * Ïƒ^2))
    
    particle.weight = weight

# Normalize weights
total = sum(p.weight for p in particles)
for p in particles:
    p.weight /= total
```

#### 4. **Resampling**

Multinomial resampling with GPS integration (bonus):
```python
# 70% particles resampled based on weights
cumulative_weights = cumsum([p.weight for p in particles])

new_particles = []
for i in range(0.7 * N):
    r = random(0, 1)
    idx = bisect.bisect_left(cumulative_weights, r)
    new_particle = particles[idx].copy_with_noise()
    new_particles.append(new_particle)

# 30% random particles (80% GPS-guided if available)
for i in range(0.3 * N):
    if gps_available and random() < 0.8:
        # GPS-guided particle
        new_particle = Particle(
            x=gps_x + random_normal(0, 0.02),
            y=gps_y + random_normal(0, 0.02),
            theta=gps_heading + random_normal(0, 0.0005)
        )
    else:
        # Fully random particle
        new_particle = Particle(random_position())
    new_particles.append(new_particle)

particles = new_particles
```

#### 5. **Pose Estimation**
```python
# Weighted average for position
x_est = sum(p.x * p.weight for p in particles)
y_est = sum(p.y * p.weight for p in particles)

# Circular mean for heading
heading_sin = sum(sin(p.Î¸) * p.weight for p in particles)
heading_cos = sum(cos(p.Î¸) * p.weight for p in particles)
Î¸_est = atan2(heading_sin, heading_cos)
```

---

## ğŸš€ Installation

### Prerequisites
```bash
# System requirements
Ubuntu 22.04
ROS2 Humble
Gazebo 11
Python 3.8+
```

### ROS2 Package Dependencies
```bash
sudo apt install ros-humble-ros-control \
                 ros-humble-effort-controllers \
                 ros-humble-joint-state-controller \
                 ros-humble-ackermann-msgs
```

### Python Dependencies
```bash
pip install -r requirements.txt
```

**requirements.txt:**
```
numpy>=1.21.0
scipy>=1.7.0
matplotlib>=3.4.0
```

### Build Instructions
```bash
# Clone repository into ROS2 workspace
cd ~/ros2_ws/src
git clone https://github.com/ansh1113/ece484-particle-filter-localization.git mp3

# Build workspace
cd ~/ros2_ws
colcon build --symlink-install

# Source workspace
source install/setup.bash
```

---

## ğŸ’» Usage

### Quick Start

**Terminal 1: Launch Gazebo Simulation**
```bash
source ~/ros2_ws/install/setup.bash
ros2 launch mp3 gem_vehicle.launch.py
```

**Terminal 2: Start Vehicle Controller**
```bash
cd ~/ros2_ws/src/mp3/src
python3 vehicle.py
```

**Terminal 3: Run Particle Filter**
```bash
cd ~/ros2_ws/src/mp3/src
python3 main.py --particles 500 --sensor_limit 15 --extensive_lidar
```

### Command Line Arguments
```bash
python3 main.py --help

Options:
  --particles N          Number of particles (default: 500)
  --sensor_limit D       Max sensor range in meters (default: 15)
  --show_frequency F     Visualization update rate (default: 10)
  --extensive_lidar      Use 8 directions instead of 4
  --gps_x_std Ïƒ         GPS noise std dev in x (default: 0)
  --gps_y_std Ïƒ         GPS noise std dev in y (default: 0)
  --gps_heading_std Ïƒ   GPS heading noise std dev (default: 0)
  --gps_update N        GPS update frequency Hz (default: 0, disabled)
```

### Setting Custom Start Position
```bash
# Reset vehicle to custom pose
python3 set_pos.py --x -45 --y 40 --heading 0

# Then run alternate waypoint sequence
python3 vehicle.py --alternate
```

### Example Configurations

**Standard Configuration (250 particles):**
```bash
python3 main.py --particles 250 --sensor_limit 15 --extensive_lidar
```

**High Accuracy (1500 particles):**
```bash
python3 main.py --particles 1500 --sensor_limit 20 --extensive_lidar
```

**With GPS Noise (Bonus Problem):**
```bash
python3 main.py --particles 1500 \
                --sensor_limit 15 \
                --gps_x_std 5 \
                --gps_y_std 5 \
                --gps_heading_std 0.393 \
                --gps_update 1 \
                --extensive_lidar
```

---

## ğŸ“Š Experimental Results

### Experiment 1: Effect of Particle Count

| Particles | Convergence Speed | Final Accuracy | Notes |
|-----------|------------------|----------------|-------|
| 250       | Moderate         | Good           | Best approximation with fewer particles |
| 750       | Fast             | Better         | Practical balance for real-time |
| 1500      | Fastest          | Best           | Highest accuracy, more stable |

**Key Finding**: Estimation accuracy improves with more particles following ~1/sqrt(N) relationship. 1500 particles provide best accuracy with stable convergence, while 750 offers practical balance between accuracy and computation.

### Experiment 2: Effect of Sensor Range

| Sensor Limit (m) | Accuracy | Convergence | Notes |
|------------------|----------|-------------|-------|
| 15               | Good     | Moderate    | Baseline configuration |
| 20               | Better   | Faster      | More informative features |
| 25               | Best     | Fastest     | Reduced ambiguity in open spaces |

**Key Finding**: Larger sensor limits increase accuracy when informative features are within range. However, excessively large limits may increase error if measurements hit max range frequently without detecting walls.

### Experiment 3: 4 vs 8 Measurement Directions

| Directions | Convergence Speed | Accuracy | Computational Cost |
|------------|------------------|----------|--------------------|
| 4          | Slower           | Good     | 1.0x (baseline)    |
| 8          | Faster           | Better   | 1.8x               |

**Key Finding**: 8 directional measurements significantly improve both convergence speed and estimation accuracy. The additional computational cost (1.8x) is justified by the performance gains.

### Experiment 4: Different Starting Positions

**Default Start (x=0, y=-98):**
- Normal convergence behavior
- Particles spread and converge smoothly

**Alternative Start (x=-45, y=40):**
- Slower convergence observed
- **Root Cause**: Vehicle collides with wall and gets stuck
- **Effect**: Motion model moves particles, but real position doesn't change (slip effect)
- **Lesson**: Physical constraints significantly impact filter performance

### Experiment 5: Measurement Noise (Bonus)

**Configuration**: 50% LiDAR dropout rate

**Observations**:
- Convergence slowed significantly
- Temporarily increased weight for incorrect particle states
- System eventually self-corrects but requires more iterations
- Highlights importance of reliable sensor data

### Performance by Environment Region

| Region        | Avg Error | Notes |
|---------------|-----------|-------|
| Corridors     | Low       | Strong geometric constraints |
| Intersections | Medium    | Multiple valid hypotheses |
| Open Spaces   | Higher    | Fewer distinctive features |
| Near Landmarks| Lowest    | Unique identifiable features |

**Key Finding**: Particle filter performs unevenly across environments. Regions with distinctive landmarks (corridors, corners) provide stronger measurement updates and lower prediction error.

---

## ğŸ¥ Demo Videos

**Video Links**: [Google Drive - MP3 Demos](https://drive.google.com/drive/folders/1tji1z8HDMwo6BdeU8uxJp2NpQ15aoZ15)

### Visualization

The turtle graphics window shows:
- ğŸŸ¢ **Green turtle**: Ground truth robot position
- ğŸŸ¡ **Yellow turtle**: Estimated position (weighted particle mean)
- ğŸ”µ **Blue arrows**: Individual particles (pose + heading + weight)
- â¬› **Black**: Walls/obstacles  
- â¬œ **White**: Free space

**Particle color intensity** indicates weight (darker = higher weight)

---

## ğŸ“š Course Context

**Course**: ECE 484 - Principles of Safe Autonomy  
**Institution**: University of Illinois Urbana-Champaign  
**Semester**: Fall 2025  
**Project Type**: Machine Problem 3 (Group Project)  
**Team**: Autoshield

---

## ğŸ‘¥ Team

**Team Autoshield**
- Ansh Bhansali - anshb3@illinois.edu
- Het Patel - hcp4@illinois.edu
- Sunny Deshpande - sunnynd2@illinois.edu
- Keisuke Ogawa - ogawa3@illinois.edu

---

## ğŸ“ Project Structure
```
ece484-particle-filter-localization/
â”œâ”€â”€ particle_filter.py       # Main MCL implementation
â”œâ”€â”€ lidar_processing.py      # 8-direction LiDAR processing
â”œâ”€â”€ maze.py                  # Map, Particle, Robot classes
â”œâ”€â”€ main.py                  # Entry point with argument parsing
â”œâ”€â”€ vehicle.py               # Vehicle controller
â”œâ”€â”€ controller.py            # Pure pursuit lateral control
â”œâ”€â”€ set_pos.py              # Utility to set vehicle position
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md               # This file
â””â”€â”€ launch/
    â””â”€â”€ gem_vehicle.launch.py  # ROS2 launch file
```

---

## ğŸ› ï¸ Technical Implementation Highlights

### Sensor Model (8-Direction Ray-Casting)
```python
def sensor_model(coordinates, sensor_limit, orientation):
    """
    Ray-cast in 8 directions to measure distances
    
    Directions:
    - 0Â°: Front
    - 45Â°: Front-Right
    - 90Â°: Right
    - 135Â°: Rear-Right
    - 180Â°: Rear
    - 225Â°: Rear-Left
    - 270Â°: Left
    - 315Â°: Front-Left
    """
    directions = [0, Ï€/4, Ï€/2, 3Ï€/4, Ï€, 5Ï€/4, 3Ï€/2, 7Ï€/4]
    distances = []
    
    for angle in directions:
        distance = 0
        dx = cos(orientation + angle)
        dy = sin(orientation + angle)
        
        while not collide_wall(x, y) and distance < sensor_limit:
            x += dx
            y += dy
            distance += 1
        
        distances.append(distance * 100)  # Convert to cm
    
    return distances
```

### GPS Integration (Bonus Problem)

To prevent particle depletion and improve robustness, we implemented a hybrid resampling strategy:

**70% Weighted Resampling** - Based on particle weights  
**30% Random Particles**:
- 80% GPS-guided (when available)
- 20% Fully random (exploration)

This prevents convergence to local minima and maintains particle diversity.

---

## ğŸ“– Academic Integrity Statement

This repository contains coursework from ECE 484 - Principles of Safe Autonomy at UIUC.  
Shared for portfolio and educational purposes after course completion.

**If you are currently enrolled in this course:**
- âŒ Do NOT copy this code for your assignments
- âœ… Use only as a learning reference
- âœ… Follow your course's academic integrity policy

Violations of academic integrity policies will be reported.

---

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- ECE 484 course staff for infrastructure and guidance
- UIUC Robotics Lab for Gazebo ECEB environment
- Team Autoshield members for collaboration

---

## ğŸ“ Contact

For questions about this implementation:
- **Ansh Bhansali**: anshbhansali5@gmail.com
- **GitHub**: [@ansh1113](https://github.com/ansh1113)

---

**â­ If you find this helpful, please star the repository!**
